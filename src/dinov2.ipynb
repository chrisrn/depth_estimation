{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a408c5-2c16-4beb-af5d-748a96efad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, DPTForDepthEstimation\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509bc05-f3eb-4b62-bc6b-f068703fd90d",
   "metadata": {},
   "source": [
    "Read image and target from train dataset using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67779042-7c95-43e8-8c78-8b50531d5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../data/nyu2_train/basement_0001a_out/1.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "mask = cv2.imread('../data/nyu2_train/basement_0001a_out/1.png')\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ea767-ad89-41f4-abc4-81117789eeb5",
   "metadata": {},
   "source": [
    "Load dinov2 model pre-trained on nyu dataset from Hugging Face and process inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2198cefc-6cc5-4e27-b7ab-0ba050c9a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crountos/Desktop/interviews/depth_estimation/venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dpt-dinov2-base-nyu\")\n",
    "model = DPTForDepthEstimation.from_pretrained(\"facebook/dpt-dinov2-base-nyu\")\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = image_processor(images=img, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acab62-efbd-4a0b-bdb1-e5288262fba5",
   "metadata": {},
   "source": [
    "Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ada69-a339-4186-a887-eda2a17ef320",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    # interpolate to original size\n",
    "    preds = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=img.shape[:2],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5461c-a0dc-4a21-b72d-256b90f97bd9",
   "metadata": {},
   "source": [
    "Structural similarity between ground truth mask and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f04334-e6d8-482f-9be9-4bfab049119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(arr):\n",
    "    min_value, max_value = np.min(arr), np.max(arr)\n",
    "    return (arr - min_value) / (max_value - min_value)\n",
    "\n",
    "preds_n = preds.squeeze().numpy()\n",
    "preds_n = scale(preds_n)\n",
    "mask_n = scale(mask)\n",
    "ssim = structural_similarity(preds_n, mask_n, data_range=1.0)\n",
    "ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f69bf7-2151-4943-bb7a-b65570bca806",
   "metadata": {},
   "source": [
    "Plot target-prediction depth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc27a2-f411-4969-9a32-5405b2b04338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colored_depthmap(depth, d_min=None, d_max=None,cmap=plt.cm.inferno):\n",
    "    if d_min is None:\n",
    "        d_min = np.min(depth)\n",
    "    if d_max is None:\n",
    "        d_max = np.max(depth)\n",
    "    depth_relative = (depth - d_min) / (d_max - d_min)\n",
    "    return 255 * cmap(depth_relative)[:,:,:3]\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_vals(imgs, targets, preds, n=1,figsize=(17,17),title=''):\n",
    "    plt.figure(figsize=figsize,dpi=150)\n",
    "    r = 2 if n == 4 else 8\n",
    "    c = 2\n",
    "    for i,idx in enumerate(np.random.randint(0,imgs.size(0),(n,))):\n",
    "        ax = plt.subplot(r,c,i + 1)\n",
    "        img,pred,gt = imgs[idx], preds[idx], targets[idx]\n",
    "        \n",
    "        pred, gt =  pred.permute(1,2,0).numpy(), gt.permute(1,2,0).numpy()\n",
    "        pred = colored_depthmap(np.squeeze(pred))\n",
    "        gt = colored_depthmap(np.squeeze(gt))\n",
    "        \n",
    "        image_viz = np.hstack([img, gt, pred])\n",
    "        plt.imshow(image_viz.astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "    title = f'{title}\\nimage/target/prediction' if len(title)!=0 else 'image/target/prediction'\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_vals(\n",
    "        torch.from_numpy(img).unsqueeze(0),\n",
    "        torch.from_numpy(mask).unsqueeze(0).unsqueeze(0),\n",
    "        preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351f680-22c6-4f0e-889e-cd8ace6bfd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
